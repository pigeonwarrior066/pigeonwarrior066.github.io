<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="width=device-width, initial-scale=1">
    <meta name="description" content="LoFTR can extract high-quality semi-dense matches even in indistinctive regions with low-textures, motion blur, or repetitive patterns. Accepted in CVPR 2021."/>
    <title>3D-Yoga: A 3D Yoga Dataset for Hierarchical Sports</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
    <style>
      body {
        background: #ffffff no-repeat fixed top left;
        font-family:'Open Sans', sans-serif;
      }
    </style>

  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col">
            <h2>3D-Yoga: A 3D Yoga Dataset for Hierarchical Sports <br> Action Analysis</h2>
            <h4 style="color:#6e6e6e;"> ECCV 2022 </h4>
            <!--author-->
            <!--<hr>
            <!--<h6> <a>author1</a><sup>1</sup>, 
                 <a>author2</a><sup>2</sup>, 
                 <a>author3</a><sup>3</sup>, 
                 <a>author4</a><sup>4</sup></h6> 
            <p> <sup>1</sup>organization1 &nbsp;&nbsp; 
                <sup>2</sup>organization2
                <br>
                <sup>*</sup> denotes equal contribution
            </p>-->
  <!-- don't need botton yet-->
            <!--<div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="pigeonwarrior066.github.io"  target="_blank">
                    <i class="fa fa-file"></i> Paper (Coming soon)</a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" id="code_soon" href="pigeonwarrior066.github.io" role="button" 
                    target="_blank" disabled>
                <i class="fa fa-github-alt"></i> data (Coming soon) </a> </p>
              </div>
            </div>-->  
           
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <hr style="margin-top:0px">
            <div><b style="color:#fd5638; font-size:large" id="demo-warning"></b>
              <br>
          <p class="text-justify">
            Visual-based human action analysis is an important research topic in the field of computer vision, and has great application prospect in sports performance analysis. Currently available 3D action analysis dtasets have a number of
            limitations in sports application, including the lack of special sports actions, distinct class or score labels and variety of samples. Existing researches mainly use
            various special RGB videos for sports action analysis, but analysis with 2D features has less effectiveness than 3D representation. In this paper, we introduce a
            new 3D yoga pose dataset (3D-Yoga) with more than 3,792 action samples and 16,668 RGB-D key frames, collected from 22 subjects performing 117 kinds of
            yoga poses with two RGB-D cameras. We have reconstructed 3D yoga poses with multi-view data and carried out experiments with a cascade two-stream adaptive
            graph convolutional neural network (Cascade 2S-AGCN) to recognize and assess these poses. Experimental results have shown the advantage of applying hierarchical analysis method on the proposed 3D-Yoga dataset. The introduction of
            3D-Yoga will enable the community to apply, develop and adapt various deep
            learning techniques for the task of visual-based sports activity analysis.
          </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- Data organization -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3> Data organization</h3>
            <hr style="margin-top:0px">
            <!-- <div class="embed-responsive embed-responsive-16by9"> -->
                <!-- <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://www.youtube.com/embed/EpmnpwwaR14" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> -->
            <!-- </div> -->
                <img class="img-fluid" src="images/file.png" alt="">
            <hr>
            <p class="text-justify">
              The organization of 3D-Yoga is shown in Fig. 3. Under the Scene directory, there are
              three folders: Front, Side and Docs. Under Docs, we provide filelist, pose score and
              camera calibration information. Inside the Front and Side folders, there are 22 folders 
              for each of the 22 subjects respectively.The folders of 7 male subjects are represented as M01, M02,..M07, 
              and the folders of 15 female subjects are represented as F01, F02,..F15. The action number 
              (A01, A02,...A10) represents the folders name of classification I, and the pose ID (a01, a03,...a117) 
              represents the folders name of classification II. In each sub-pose folder corresponding to each motion segment, there are
              three sub-folders, i.e., Color, Depth and Skeleton.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>
    
    <!-- Design of the two-level hierarchical classification for yoga poses -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3> Design of the two-level hierarchical classification for yoga poses</h3>
            <hr style="margin-top:0px">
            <!-- <div class="embed-responsive embed-responsive-16by9"> -->
                <!-- <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://www.youtube.com/embed/EpmnpwwaR14" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> -->
            <!-- </div> -->
                <img class="img-fluid" src="images/pic3.jpg" alt="">
            <hr>
            <p class="text-justify">
              Each subject performs yoga poses in accordance with four sets of exercises in daily
              yoga, there are 158 categories of yoga poses in total. Since some yoga poses are repeated, such as hunker pranayama appearing 
              three times, we merge the same movements. The final category of yoga poses in 3D-Yoga is adjusted to 117, and each pose
              is different but covered all yoga formulas. Hierarchical annotations can benefit deep
              learning networks because they provide users with rich information of not only pose
              names but also body postures (standing, bending, etc.).  we design a two-level hierarchy to organize these 117 categories of yoga poses, 
              in which the first level has 10 categories and the second level are the sub-categories of yoga poses. For
              the first level classification, we provide their names, detailed definitions, corresponding
              labels of second-level, and example poses. The second level classification is the detailed
              division of first level classification.         
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>
    
    <!--  Samples of 3D-Yoga dataset -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3> Samples of 3D-Yoga dataset</h3>
            <hr style="margin-top:0px">
            <!-- <div class="embed-responsive embed-responsive-16by9"> -->
                <!-- <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://www.youtube.com/embed/EpmnpwwaR14" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> -->
            <!-- </div> -->
                <img class="img-fluid" src="images/pic2.jpg" alt="">
                <img class="img-fluid" src="images/pic1.jpg" alt="">
            <hr>
            <p class="text-justify">
              Here are picture groups of examples in our dataset.The first picture group shows different poses of female object under the conditions of Partial-light and High-light.
              And the second shows different poses of male object under the conditions of Low-light.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- ack -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Acknowledgements</h3>
          <hr style="margin-top:0px">
          <p class="text-justify">
            We would like to thank you for insightful and constructive comments. 
          </p>
          <hr>
      </div>
    </div>
  </div>


  <footer class="text-center" style="margin-bottom:10px; font-size: medium;">
      <hr>
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
  </footer>

  
  <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>
